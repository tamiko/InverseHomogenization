/**
*
* Copyright (C) 2012-2023 by the DOpElib authors
*
* This file is part of DOpElib
*
* DOpElib is free software: you can redistribute it
* and/or modify it under the terms of the GNU General Public
* License as published by the Free Software Foundation, either
* version 3 of the License, or (at your option) any later
* version.
*
* DOpElib is distributed in the hope that it will be
* useful, but WITHOUT ANY WARRANTY; without even the implied
* warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
* PURPOSE.  See the GNU General Public License for more
* details.
*
* Please refer to the file LICENSE.TXT included in this distribution
* for further information on this license.
*
**/

#ifndef REDUCEDGRADIENTDESCENT_ALGORITHM_H_
#define REDUCEDGRADIENTDESCENT_ALGORITHM_H_

DEAL_II_DISABLE_EXTRA_DIAGNOSTICS
#include <opt_algorithms/reducedalgorithm.h>
#include <include/parameterreader.h>
DEAL_II_ENABLE_EXTRA_DIAGNOSTICS

#include <iostream>
#include <assert.h>
#include <iomanip>
namespace DOpE
{
  /**
   * @class ReducedGradientDescentAlgorithm
   *
   * This class provides a solver for equality constrained optimization
   * problems in reduced form, i.e., the dependent variable is
   * assumed to be eliminated by solving the equation. I.e.,
   * we solve the problem min j(q)
   *
   * The solution is done with a linesearch algorithm, see, e.g.,
   * Nocedal & Wright.
   *
   * @tparam <PROBLEM>    The problem container. See, e.g., OptProblemContainer
   * @tparam <VECTOR>     The vector type of the solution.
   */
  template <typename PROBLEM, typename VECTOR>
  class ReducedGradientDescentAlgorithm : public ReducedAlgorithm<PROBLEM, VECTOR>
  {
  public:
    /**
     * The constructor for the algorithm
     *
     * @param OP              A pointer to the problem container
     * @param S               The reduced problem. This object handles the equality
     *                        constraint. For the interface see ReducedProblemInterface.
     * @param param_reader    A parameter reader to access user given runtime parameters.
     * @param Except          The DOpEExceptionHandler. This is used to handle the output
     *                        by all exception.
     * @param Output          The DOpEOutputHandler. This takes care of all output
     *                        generated by the problem.
     * @param base_priority   An offset for the priority of the output generated by the algorithm.
     */
    ReducedGradientDescentAlgorithm(PROBLEM *OP,
                           ReducedProblemInterface<PROBLEM, VECTOR> *S,
                           ParameterReader &param_reader,
                           DOpEExceptionHandler<VECTOR> *Except=NULL,
                           DOpEOutputHandler<VECTOR> *Output=NULL,
                           int base_priority=0);
    virtual ~ReducedGradientDescentAlgorithm();

    /**
     * Used to declare run time parameters. This is needed to declare all
     * parameters a startup without the need for an object to be already
     * declared.
     */
    static void declare_params(ParameterReader &param_reader);

    /**
     * This solves an Optimizationproblem in only the control variable
     * by a newtons method.
     *
     * @param q           The initial point.
     * @param global_tol  An optional parameter specifying the required  tolerance.
     *                    The actual tolerance is the maximum of this and the one specified in the param
     *                    file. Its default value is negative, so that it has no influence if not specified.
     */
    virtual int Solve(ControlVector<VECTOR> &q,double global_tol=-1.);
    /**
     * This returns the natural norm of the newton residual. This means the norm of the gradient of the
     * reduced cost functional.
     *
     * @param q           The initial point.
     */
    double GradientDescentResidual(const ControlVector<VECTOR> &q);

    unsigned int &get_nonlinear_maxiter() { return nonlinear_maxiter_; }

  protected:
    /**
     * Performs an Armijo-type linesearch to find a point of sufficient descent
     * for the functional j along the direction dq.
     *
     *
     * @param dq                    The search direction.
     * @param gradient              The l^2 gradient of the costfunctional at q,
     *                              i.e., the gradient_i = \delta_{q_i} j(q)
     *                              where q_i denotes the i-th DoF for the control.
     * @param q                     The control. Needs to be the last evaluation point of
     *                              j in the begining and is at the end the updated
     *                              control q+\alpha dq.
     */
    virtual int ReducedGradientDescentLineSearch(const ControlVector<VECTOR> &dq,
                                        const ControlVector<VECTOR> &gradient,
                                        double &cost,
                                        ControlVector<VECTOR> &q);
    /**
     * Evaluates the squared residual, i.e., the scalar product gradient*gradient_transposed
     */
    virtual double Residual(const ControlVector<VECTOR> &gradient,
                            const ControlVector<VECTOR> &gradient_transposed)
    {
      return  gradient*gradient_transposed;
    }

  private:
    unsigned int nonlinear_maxiter_, line_maxiter_;
    double       nonlinear_tol_, nonlinear_global_tol_, lineasearch_rho_, linesearch_c_;
    bool         compute_functionals_in_every_step_;
    std::string postindex_;
  };

  /***************************************************************************************/
  /****************************************IMPLEMENTATION*********************************/
  /***************************************************************************************/
  using namespace dealii;

  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  void ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::declare_params(ParameterReader &param_reader)
  {
    param_reader.SetSubsection("reducedgradientdescentalgorithm parameters");
    param_reader.declare_entry("nonlinear_maxiter", "10",Patterns::Integer(0));
    param_reader.declare_entry("nonlinear_tol", "1.e-7",Patterns::Double(0));
    param_reader.declare_entry("nonlinear_global_tol", "1.e-11",Patterns::Double(0));

    param_reader.declare_entry("line_maxiter", "10",Patterns::Integer(0));
    param_reader.declare_entry("linesearch_rho", "0.9",Patterns::Double(0));
    param_reader.declare_entry("linesearch_c", "0.1",Patterns::Double(0));

    param_reader.declare_entry("compute_functionals_in_every_step", "false",Patterns::Bool());

    ReducedAlgorithm<PROBLEM, VECTOR>::declare_params(param_reader);
  }
  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::ReducedGradientDescentAlgorithm(PROBLEM *OP,
      ReducedProblemInterface<PROBLEM, VECTOR> *S,
      ParameterReader &param_reader,
      DOpEExceptionHandler<VECTOR> *Except,
      DOpEOutputHandler<VECTOR> *Output,
      int base_priority)
    : ReducedAlgorithm<PROBLEM, VECTOR>(OP,S,param_reader,Except,Output,base_priority)
  {

    param_reader.SetSubsection("reducedgradientdescentalgorithm parameters");
    nonlinear_maxiter_    = param_reader.get_integer ("nonlinear_maxiter");
    nonlinear_tol_        = param_reader.get_double ("nonlinear_tol");
    nonlinear_global_tol_ = param_reader.get_double ("nonlinear_global_tol");

    line_maxiter_         = param_reader.get_integer ("line_maxiter");
    lineasearch_rho_       = param_reader.get_double ("linesearch_rho");
    linesearch_c_         = param_reader.get_double ("linesearch_c");

    compute_functionals_in_every_step_  = param_reader.get_bool ("compute_functionals_in_every_step");

    postindex_ = "_"+this->GetProblem()->GetName();
  }

  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::~ReducedGradientDescentAlgorithm()
  {

  }

  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  double ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::GradientDescentResidual(const ControlVector<VECTOR> &q)
  {
    //Solve j'(q) = 0
    ControlVector<VECTOR> gradient(q), gradient_transposed(q);

    try
      {
        this->GetReducedProblem()->ComputeReducedCostFunctional(q);
      }
    catch (DOpEException &e)
      {
        this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::GradientDescentResidual");
      }

    try
      {
        this->GetReducedProblem()->ComputeReducedGradient(q,gradient,gradient_transposed);
      }
    catch (DOpEException &e)
      {
        this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::GradientDescentResidual");
      }

    return sqrt(Residual(gradient,gradient_transposed));
  }

  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  int ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::Solve(ControlVector<VECTOR> &q,double global_tol)
  {

    q.ReInit();
    //Solve j'(q) = 0
    ControlVector<VECTOR> dq(q), gradient(q), gradient_transposed(q);

    unsigned int iter=0;
    double cost=0.;
    std::stringstream out;
    this->GetOutputHandler()->InitNewtonOut(out);

    out << "**************************************************\n";
    out << "*        Starting Reduced GradientDescent Algorithm       *\n";
    out << "*   Solving : "<<this->GetProblem()->GetName()<<"\t*\n";
    out << "*  CDoFs : ";
    q.PrintInfos(out);
    out << "*  SDoFs : ";
    this->GetReducedProblem()->StateSizeInfo(out);
    out << "**************************************************";
    this->GetOutputHandler()->Write(out,1+this->GetBasePriority(),1,1);

    this->GetOutputHandler()->SetIterationNumber(iter,"OptGradientDescent"+postindex_);

    this->GetOutputHandler()->Write(q,"Control"+postindex_,"control");

    try
      {
        cost = this->GetReducedProblem()->ComputeReducedCostFunctional(q);
      }
    catch (DOpEException &e)
      {
        this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::Solve");
      }

    out<< "CostFunctional: " << cost;
    this->GetOutputHandler()->Write(out,2+this->GetBasePriority());

    if (compute_functionals_in_every_step_ == true)
      {
        try
          {
            this->GetReducedProblem()->ComputeReducedFunctionals(q);
          }
        catch (DOpEException &e)
          {
            this->GetExceptionHandler()->HandleCriticalException(e);
          }
      }

    try
      {
        this->GetReducedProblem()->ComputeReducedGradient(q,gradient,gradient_transposed);
      }
    catch (DOpEException &e)
      {
        this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::Solve");
      }

    double res = Residual(gradient,gradient_transposed);//gradient*gradient_transposed;
    double firstres = res;

    assert(res >= 0);

    this->GetOutputHandler()->Write(gradient,"GradientDescentResidual"+postindex_,"control");
    out<< "\t GradientDescent step: " <<iter<<"\t Residual (abs.): "<<sqrt(res)<<"\n";
    out<< "\t GradientDescent step: " <<iter<<"\t Residual (rel.): "<<std::scientific<<sqrt(res)/sqrt(res)<<"\n";
    this->GetOutputHandler()->Write(out,3+this->GetBasePriority());
    int lineiter =0;
    unsigned int miniter = 0;
    if (global_tol > 0.)
      miniter = 1;

    global_tol =  std::max(nonlinear_global_tol_,global_tol);
    while (( (res >= global_tol*global_tol) && (res >= nonlinear_tol_*nonlinear_tol_*firstres) ) ||  iter < miniter )
      {
        iter++;
        this->GetOutputHandler()->SetIterationNumber(iter,"OptGradientDescent"+postindex_);

        if (iter > nonlinear_maxiter_)
          {
            throw DOpEIterationException("Iteration count exceeded bounds!","ReducedGradientDescentAlgorithm::Solve");
          }

        //Compute a search direction
	//Use the correct gradient! (here called _transposed) The usual l^2 representation
	//is typically very bad!
	dq = gradient_transposed;
	dq *= -1.;
        //Linesearch
        try
          {
            lineiter = ReducedGradientDescentLineSearch(dq,gradient,cost,q);
          }
        catch (DOpEIterationException &e)
          {
            //Seems uncritical too many line search steps, it'll probably work
            //So only write a warning, and continue.
            this->GetExceptionHandler()->HandleException(e,"ReducedGradientDescentAlgorithm::Solve");
            lineiter = -1;
          }
        //catch(DOpEException& e)
        //{
        //  this->GetExceptionHandler()->HandleCriticalException(e);
        //}

        out<< "CostFunctional: " << cost;
        this->GetOutputHandler()->Write(out,3+this->GetBasePriority());

        if (compute_functionals_in_every_step_ == true)
          {
            try
              {
                this->GetReducedProblem()->ComputeReducedFunctionals(q);
              }
            catch (DOpEException &e)
              {
                this->GetExceptionHandler()->HandleCriticalException(e);
              }
          }


        //Prepare the next Iteration
        try
          {
            this->GetReducedProblem()->ComputeReducedGradient(q,gradient,gradient_transposed);
          }
        catch (DOpEException &e)
          {
            this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::Solve");
          }

        this->GetOutputHandler()->Write(q,"Control"+postindex_,"control");
        this->GetOutputHandler()->Write(gradient,"GradientDescentResidual"+postindex_,"control");

        res = Residual(gradient,gradient_transposed);//gradient*gradient_transposed;

        out<<"\t GradientDescent step: " <<iter<<"\t Residual (rel.): "<<this->GetOutputHandler()->ZeroTolerance(sqrt(res)/sqrt(firstres),1.0)<< "\t LineSearch {"<<lineiter<<"} ";
        this->GetOutputHandler()->Write(out,3+this->GetBasePriority());
      }

    //We are done write total evaluation
    out<< "CostFunctional: " << cost;
    this->GetOutputHandler()->Write(out,2+this->GetBasePriority());
    try
      {
        this->GetReducedProblem()->ComputeReducedFunctionals(q);
      }
    catch (DOpEException &e)
      {
        this->GetExceptionHandler()->HandleCriticalException(e,"ReducedGradientDescentAlgorithm::Solve");
      }

    out << "**************************************************\n";
    out << "*        Stopping Reduced GradientDescent Algorithm       *\n";
    out << "*             after "<<std::setw(6)<<iter<<"  Iterations           *\n";
    out.precision(4);
    out << "*             with rel. Residual "<<std::scientific << std::setw(11) << this->GetOutputHandler()->ZeroTolerance(sqrt(res)/sqrt(firstres),1.0)<<"          *\n";
    out.precision(10);
    out << "**************************************************";
    this->GetOutputHandler()->Write(out,1+this->GetBasePriority(),1,1);
    return iter;
  }

  /******************************************************/

  template <typename PROBLEM, typename VECTOR>
  int ReducedGradientDescentAlgorithm<PROBLEM, VECTOR>::ReducedGradientDescentLineSearch(const ControlVector<VECTOR> &dq,
      const ControlVector<VECTOR>  &gradient,
      double &cost,
      ControlVector<VECTOR> &q)
  {
    double rho = lineasearch_rho_;
    double c   = linesearch_c_;

    double costnew = 0.;
    bool force_linesearch = false;

    q+=dq;
    try
      {
        costnew = this->GetReducedProblem()->ComputeReducedCostFunctional(q);
      }
    catch (DOpEException &e)
      {
//    this->GetExceptionHandler()->HandleException(e);
        force_linesearch = true;
        this->GetOutputHandler()->Write("Computing Cost Failed",4+this->GetBasePriority());
      }

    double alpha=1;
    unsigned int iter =0;

    double reduction = gradient*dq;
    if (reduction > 0)
      {
        this->GetOutputHandler()->WriteError("Waring: computed direction doesn't seem to be a descend direction!");
        reduction = 0;
      }

    if (line_maxiter_ > 0)
      {
        if (fabs(reduction) < 1.e-10*cost)
          reduction = 0.;
        if (std::isinf(costnew) || std::isnan(costnew) || (costnew >= cost + c*alpha*reduction) || force_linesearch)
          {
            this->GetOutputHandler()->Write("\t linesearch ",4+this->GetBasePriority());
            while (std::isinf(costnew) || std::isnan(costnew) || (costnew >= cost + c*alpha*reduction) || force_linesearch)
              {
                iter++;
                if (iter > line_maxiter_)
                  {
                    if (force_linesearch)
                      {
                        throw DOpEException("Iteration count exceeded bounds while unable to compute the CostFunctional!","ReducedGradientDescentAlgorithm::ReducedGradientDescentLineSearch");
                      }
                    else
                      {
                        cost = costnew;
                        throw DOpEIterationException("Iteration count exceeded bounds!","ReducedGradientDescentAlgorithm::ReducedGradientDescentLineSearch");
                      }
                  }
                force_linesearch = false;
                q.add(alpha*(rho-1.),dq);
                alpha *= rho;

                try
                  {
                    costnew = this->GetReducedProblem()->ComputeReducedCostFunctional(q);
                  }
                catch (DOpEException &e)
                  {
                    //this->GetExceptionHandler()->HandleException(e);
                    force_linesearch = true;
                    this->GetOutputHandler()->Write("Computing Cost Failed",4+this->GetBasePriority());
                  }
              }
          }
        cost = costnew;
      }

    return iter;

  }


}
#endif
